{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367c1965-259d-49dd-9223-5d64f5856da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "from dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ed0ad3-064c-410c-8791-08cd870ef619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lr_utils import load_dataset\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f759e4b1-9603-49cd-8e9c-406042ab0f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, 64, 64, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5191b643-b7bf-440e-ba38-644db384f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = test_set_x_orig[:,1].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23052035-61cb-452f-83e8-7d142909df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_flatten = train_set_x_orig.reshape(m_train, -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(m_test, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78fdaa71-e4f7-498a-ac23-ca1a04f94c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x = test_set_x_flatten / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e87d1abf-2847-4ed2-8392-3fb17a258cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_set_x\n",
    "Y = train_set_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b21aa5-0b64-4ce9-9920-f5f23546c96a",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Outline\n",
    "\n",
    "To build your neural network, you'll be implementing several \"helper functions.\" These helper functions will be used in the next assignment to build a two-layer neural network and an L-layer neural network. \n",
    "\n",
    "Each small helper function will have detailed instructions to walk you through the necessary steps. Here's an outline of the steps in this assignment:\n",
    "\n",
    "- Initialize the parameters for a two-layer network and for an $L$-layer neural network\n",
    "- Implement the forward propagation module (shown in purple in the figure below)\n",
    "     - Complete the LINEAR part of a layer's forward propagation step (resulting in $Z^{[l]}$).\n",
    "     - The ACTIVATION function is provided for you (relu/sigmoid)\n",
    "     - Combine the previous two steps into a new [LINEAR->ACTIVATION] forward function.\n",
    "     - Stack the [LINEAR->RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR->SIGMOID] at the end (for the final layer $L$). This gives you a new L_model_forward function.\n",
    "- Compute the loss\n",
    "- Implement the backward propagation module (denoted in red in the figure below)\n",
    "    - Complete the LINEAR part of a layer's backward propagation step\n",
    "    - The gradient of the ACTIVATE function is provided for you(relu_backward/sigmoid_backward) \n",
    "    - Combine the previous two steps into a new [LINEAR->ACTIVATION] backward function\n",
    "    - Stack [LINEAR->RELU] backward L-1 times and add [LINEAR->SIGMOID] backward in a new L_model_backward function\n",
    "- Finally, update the parameters\n",
    "\n",
    "<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center><b>Figure 1</b></center></caption><br>\n",
    "\n",
    "\n",
    "**Note**:\n",
    "\n",
    "For every forward function, there is a corresponding backward function. This is why at every step of your forward module you will be storing some values in a cache. These cached values are useful for computing gradients. \n",
    "\n",
    "In the backpropagation module, you can then use the cache to calculate the gradients. Don't worry, this assignment will show you exactly how to carry out each of these steps! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1877d072-f0f3-42bf-a5fe-c8e29483a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z:np.array):\n",
    "    A = np.maximum(0, Z)\n",
    "    assert A.shape == Z.shape\n",
    "    return(A, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78d79369-d8d0-47c5-9774-49eda4eac1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z:np.array):\n",
    "    denom = 1+np.exp(-(Z))\n",
    "    return(np.divide(1,denom), Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "291db167-112f-4fb7-be48-1006afa40dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_proj(X, W, b):\n",
    "    return(np.dot(W, X) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5508e8ae-d4af-47bc-9f23-21da51abd21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = train_set_x.shape[0]\n",
    "n_h = 1000\n",
    "n_y = 1\n",
    "\n",
    "layer_dims = [n_x, n_h, 50, n_y]\n",
    "def initialize(layer_dims):\n",
    "    # need to initialize all the weights and biases that you are going to fit in the model.\n",
    "    # this means you are probably going to have to set those ahead of time, since the structure of the model cannot change once it is created.\n",
    "    \n",
    "    L = len(layer_dims)\n",
    "    params = {}\n",
    "    for l in range(1,L): # skip l = 0, input\n",
    "        params['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        params['b'+str(l)] = np.random.randn(layer_dims[l],1)\n",
    "        \n",
    "        # should be clear, but check anyways\n",
    "        assert(params['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(params['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    return(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92149374-a981-4cce-92ea-b296597649ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A,W,b):\n",
    "    # needs to calculate the linear transform given the weights and biases put into it\n",
    "    # calculate actvation in first layer\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return(Z, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bfb70de8-c5fd-4d07-b6a6-faeceb916859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(Z, t:str):\n",
    "    if t == 'relu':\n",
    "        A, act_cache = relu(Z)\n",
    "    if t == 'sigmoid':\n",
    "        A, act_cache = sigmoid(Z)\n",
    "    cache = (Z, act_cache)\n",
    "    return(A, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9c965a7-6a45-4e8f-ae19-20967582d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward_activation(A_prev, W, b, t:str):\n",
    "    Z, cache = linear_forward(A_prev, W, b)\n",
    "    act_cache = []\n",
    "    A = None\n",
    "    if t == 'relu':\n",
    "        A, act_cache = relu(Z)\n",
    "    elif t == 'sigmoid':\n",
    "        A, act_cache = sigmoid(Z)\n",
    "    return(A,act_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d29bbc5-b862-41a2-8065-2871e3a52bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagation(X, layer_dims):\n",
    "    # initialize weights and biases\n",
    "    params = initialize(layer_dims) # get the weights and biases\n",
    "    \n",
    "    # for each layer calculate weights and biases by calling linear_forward_activation\n",
    "    L = len(layer_dims)\n",
    "    act_caches = []\n",
    "    A_prev = X\n",
    "    for i in range(1, L-1): # the last layer is a sigmoid\n",
    "        A_prev, cache = linear_forward_activation(A_prev, params['W'+str(i)], params['b'+str(i)], 'relu')\n",
    "        \n",
    "    # once you hit the last year, you need to output a set of sigmoids to squash the signal into 0,1 for a binary problem.\n",
    "    return(linear_forward_activation(A_prev, params['W'+str(L-1)], params['b'+str(L-1)], 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4ab48-64d6-4243-960d-2175adb027bb",
   "metadata": {},
   "source": [
    "Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd557908-3045-41ea-b78c-67129fda2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aout = propagation(X, layer_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5057da19-6cec-42d9-a8ce-92cf672785f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = -(1/m)*np.sum((Y*np.log(AL)+(1-Y)*np.log(1-AL)))\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b2f17247-aaaa-43d0-b664-49310fb4f772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.407241403857709"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(Aout[0], Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2862f68-2e4d-473a-9465-d8866a46df51",
   "metadata": {},
   "source": [
    "### forward code is complete, but now we need to go backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26bb52-c6a7-499e-bd6e-652380790ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eb4af9d-8233-4e95-9282-1a204eb3b10a",
   "metadata": {},
   "source": [
    "### Driver code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "62500591-c95e-4523-83a5-42f3897bbad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = initialize(layer_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6ca19657-4d51-4179-9ee5-c58299b7e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_forward(X, out['W1'], out['b1']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5a0657d7-b4cb-4c7f-9493-415ba3d924d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aout = propagation(X, layer_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "992f8e75-4eef-4e96-a076-7d80aae30cc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompute_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[146], line 15\u001b[0m, in \u001b[0;36mcompute_cost\u001b[0;34m(AL, Y)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_cost\u001b[39m(AL, Y):\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Implement the cost function defined by equation (7).\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    cost -- cross-entropy cost\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m     cost \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39mm)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(( np\u001b[38;5;241m.\u001b[39mmultiply(Y, np\u001b[38;5;241m.\u001b[39mlog(AL)) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mY), np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mAl) )))\n\u001b[1;32m     17\u001b[0m     cost \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(cost)      \u001b[38;5;66;03m# To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "compute_cost(Aout[0], np.array([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74887778-efa2-4c53-bb64-a8f14d54c645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
